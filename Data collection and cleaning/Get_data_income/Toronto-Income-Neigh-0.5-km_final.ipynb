{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d1746c5",
   "metadata": {},
   "source": [
    "# Main-code for House-Amneties-Neighbourhood-Crime-Income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a3686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd party imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pylab as plt\n",
    "from bs4 import BeautifulSoup\n",
    "#!pip install pandas openpyxl\n",
    "\n",
    "\n",
    "# Configure Notebook\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set_context(\"notebook\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%config Completer.use_jedi = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load CSV \n",
    "tn_df = pd.read_csv(\"Toronto-Neigh.csv\", encoding='Windows-1252')\n",
    "tnT_df=tn_df.T\n",
    "\n",
    "# View DataFrame\n",
    "tn_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0977fa",
   "metadata": {},
   "source": [
    "## Filtering and Cleaning the dataframe to obtain the required data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first data row (index 1, which is the second row of the sheet)\n",
    "first_row = tn_df.T.iloc[0].astype(str)\n",
    "\n",
    "# Keywords to search for\n",
    "keywords = [\n",
    "    'Income statistics in 2020 for the population aged 15 years and over in private households - 25% sample data',\n",
    "   # 'Income', \n",
    "    'tax',\n",
    "    'after-tax income',\n",
    "    'after-tax income in 2020',\n",
    "    'Median total income'\n",
    "]\n",
    "\n",
    "# Initialize a dictionary to store all indexes for each keyword\n",
    "all_keyword_indexes = {}\n",
    "\n",
    "# Find keywords in the first row (case-insensitive matching) and their corresponding indexes\n",
    "for keyword in keywords:\n",
    "    # Find all indexes for the keyword\n",
    "    indexes = first_row[first_row.str.contains(keyword, case=False, na=False)].index.tolist()\n",
    "    \n",
    "    # If keyword is found, store the indexes\n",
    "    if indexes:\n",
    "        # Store the indexes for the keyword\n",
    "        all_keyword_indexes[keyword] = indexes\n",
    "        \n",
    "        # Calculate and print only the min and max indexes\n",
    "        min_index = min(indexes)  # Get the minimum index\n",
    "        max_index = max(indexes)  # Get the maximum index\n",
    "        print(f\"Keyword '{keyword}' found at min index: {min_index} and max index: {max_index}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b1baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the actual values at columns 60 and 1949 (first row)\n",
    "value_at_index_60 = tn_df.T.iloc[0,92]  # 61st column, first row (index 0)\n",
    "value_at_index_1949 = tn_df.T.iloc[0, 424]  # 1950th column, first row (index 0)\n",
    "\n",
    "# Optionally, get the column names at those indices\n",
    "column_name_60 = tn_df.T.columns[92]  # 61st column name\n",
    "column_name_1949 = tn_df.T.columns[424]  # 1950th column name\n",
    "\n",
    "# Print the results\n",
    "print(f\"Value at column {column_name_60} (index 60): {value_at_index_60}\")\n",
    "print(f\"Value at column {column_name_1949} (index 1949): {value_at_index_1949}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fefeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Extract the first column (assuming it contains labels or identifiers)\n",
    "first_column = tn_df.T.iloc[:, 0]\n",
    "\n",
    "# Step 2: Filter the columns based on index range (between 60 and 168)\n",
    "filtered_columns = tn_df.T.iloc[:, 60:167]  # Select columns from index 60 to 168 (inclusive of 60, exclusive of 169)\n",
    "\n",
    "# Step 3: Create a new DataFrame with the first column and the filtered columns\n",
    "filtered_df = pd.concat([first_column, filtered_columns], axis=1)\n",
    "\n",
    "# Step 4: Print the resulting filtered DataFrame\n",
    "(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f6237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Reset the row index and add it as a column (this step you already have)\n",
    "filtered_df_reset = filtered_df.reset_index()\n",
    "\n",
    "# Step 2: Set the row index column to the first column (this you already did)\n",
    "filtered_df_reset.set_index(filtered_df_reset.columns[0], inplace=True)\n",
    "\n",
    "# Step 3: Reset column headers to default (0, 1, 2, ...)\n",
    "# This step will reset the column headers (the numeric indices you mentioned)\n",
    "filtered_df_reset.columns = range(filtered_df_reset.shape[1])\n",
    "\n",
    "# Confirmation message\n",
    "print(\"Row index has been moved to the first column, and column headers have been reset.\")\n",
    "filtered_df_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Reset the index, which makes the current index a column\n",
    "filtered_df_reset1 = filtered_df_reset.reset_index()\n",
    "\n",
    "# Optional Step 2: Set a specific column as the new index (if you need a specific index)\n",
    "# df_reset.set_index('your_column_name', inplace=True)\n",
    "\n",
    "# Confirmation message\n",
    "print(\"Index column has been moved to the first column, and row index has been reset.\")\n",
    "\n",
    "filtered_df_reset1= filtered_df_reset1.rename(columns={\"index\": \"Neighbourhood\"})\n",
    "filtered_df_reset1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbebb173",
   "metadata": {},
   "source": [
    "## Loading the file from the previous pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93271e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second file\n",
    "hos_t=pd.read_csv(\"Toronto_houses_with_crime_data_0.5km.csv\")\n",
    "hos_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e32f9",
   "metadata": {},
   "source": [
    "### Combining the data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=hos_t\n",
    "df['Neighbourhood'] = df['Neighbourhood'].str.lower()\n",
    "column_name = 'Neighbourhood'\n",
    "\n",
    "# Get unique entries in the column\n",
    "unique_entries = df[column_name].dropna().unique()\n",
    "\n",
    "# Convert to a list if needed (or you can leave it as an array)\n",
    "unique_entries_list = unique_entries.tolist()\n",
    "\n",
    "# Display the unique entries\n",
    "len(unique_entries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0133c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=filtered_df_reset1\n",
    "df['Neighbourhood'] = df['Neighbourhood'].str.lower()\n",
    "column_name = 'Neighbourhood'\n",
    "\n",
    "# Get unique entries in the column\n",
    "unique_entries = df[column_name].dropna().unique()\n",
    "\n",
    "# Convert to a list if needed (or you can leave it as an array)\n",
    "unique_entries_list = unique_entries.tolist()\n",
    "\n",
    "# Display the unique entries\n",
    "len(unique_entries_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d1eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the two DataFrames\n",
    "df1 = hos_t\n",
    "df2 = filtered_df_reset1\n",
    "\n",
    "# Specify the column with unique entries in df1 and the corresponding column in df2\n",
    "unique_column_df1 = 'Neighbourhood'\n",
    "matching_column_df2 = 'Neighbourhood'\n",
    "\n",
    "# Clean the 'Neighbourhood' column in both DataFrames\n",
    "#df1[unique_column_df1] = df1[unique_column_df1].str.strip().str.lower().replace({'\\'': '', ',': '': ''}, regex=True)\n",
    "df1[unique_column_df1] = df1[unique_column_df1].str.strip().str.lower().replace({'\\'': ' ', ',': ' ', '-': ' ','`': ' '}, regex=True).str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "df2[matching_column_df2] = df2[matching_column_df2].str.strip().str.lower().replace({'\\'': ' ', ',': ' ', '-': ' ','`': ' '}, regex=True).str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "df1[unique_column_df1] = df1[unique_column_df1].str.replace(r'\\bst\\.\\b', 'st ', regex=True, case=False)\n",
    "df2[matching_column_df2] = df2[matching_column_df2].str.replace(r'st\\.', 'st ', regex=True, case=False)\n",
    "\n",
    "df1[unique_column_df1] = df1[unique_column_df1].str.replace(r'\\s+', ' ', regex=True)\n",
    "df2[matching_column_df2] = df2[matching_column_df2].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "\n",
    "# Step 1: Get unique entries from df1 (now cleaned)\n",
    "unique_entries = df1[unique_column_df1].dropna().unique()\n",
    "\n",
    "# Step 2: Filter rows in df2 where the matching column values are in unique_entries\n",
    "filtered_df2 = df2[df2[matching_column_df2].isin(unique_entries)]\n",
    "\n",
    "# Display the filtered dataframe\n",
    "filtered_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f207adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get entries in df1 that are not in df2\n",
    "unmatched_entries = set(unique_entries) - set(df2[matching_column_df2].unique())\n",
    "print(\"Unmatched entries:\", unmatched_entries)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3ea63",
   "metadata": {},
   "source": [
    "### Getting only the subset of income dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2621cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the specific row in df1 that you want to use as column headers\n",
    "# For example, if it's the first row (index 0):\n",
    "df1= filtered_df_reset\n",
    "new_headers = df1.iloc[0]\n",
    "len(new_headers)\n",
    "\n",
    "df2= filtered_df2\n",
    "# # Update df2's column names with these new headers\n",
    "# Keep the first column header of df2 unchanged\n",
    "df2_columns = [df2.columns[0]] + list(new_headers)\n",
    "\n",
    "# Update df2's columns\n",
    "df2.columns = df2_columns\n",
    "\n",
    "# # Confirm the change\n",
    "# print(\"Updated column names of df2:\")\n",
    "income_1=df2\n",
    "income_1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22138051",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Select and convert the necessary columns to lists\n",
    "columns_to_keep = ['Neighbourhood'] + ['    Average total income in 2020 among recipients ($)'] + ['    Average after-tax income in 2020 among recipients ($)'] #+ income_1.iloc[:,31:80]\n",
    "\n",
    "# Use the columns_to_keep list to select those columns from the DataFrame\n",
    "income = income_1[columns_to_keep]\n",
    "\n",
    "# Display the new DataFrame\n",
    "(income)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4cd45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#segregating gross-aftertax income's\n",
    "# Group 1: Columns 1 to N (excluding the first column)\n",
    "# Adjust the range of columns as necessary\n",
    "df_1 = income_1.iloc[:, 31:47]  # First half of remaining columns\n",
    "df_2 = income_1.iloc[:, 47:63]   # Second half of remaining columns\n",
    "df_3 = income_1.iloc[:, 63:80]   # Second half of remaining columns\n",
    "\n",
    "# # Step 3: Add the 'Neigh' column back to each new DataFrame\n",
    "df_1 = pd.concat([income,df_1], axis=1) #gross\n",
    "df_2 = pd.concat([income,df_2], axis=1) #after-tax\n",
    "df_3 = pd.concat([income,df_3],axis=1)  #employement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a509e04",
   "metadata": {},
   "source": [
    "#### Combining the income braackets to define 6 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8312479",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Gross-income of individuals\n",
    "\n",
    "# Add the values of two columns (e.g., 'column1' and 'column2') and assign the result to a new column\n",
    "avg_expense = 1530*12\n",
    "\n",
    "df_1['Class 6G'] = pd.to_numeric(df_1.iloc[:,7], errors='coerce') + pd.to_numeric(df_1.iloc[:,8], errors='coerce')\n",
    "df_1['Class 5G'] = pd.to_numeric(df_1.iloc[:,9], errors='coerce') + pd.to_numeric(df_1.iloc[:,10], errors='coerce') +pd.to_numeric(df_1.iloc[:,11], errors='coerce')\n",
    "df_1['Class 4G'] = pd.to_numeric(df_1.iloc[:,12], errors='coerce') + pd.to_numeric(df_1.iloc[:,13], errors='coerce') \n",
    "df_1['Class 3G'] = pd.to_numeric(df_1.iloc[:,14], errors='coerce') + pd.to_numeric(df_1.iloc[:,15], errors='coerce') +pd.to_numeric(df_1.iloc[:,16], errors='coerce')\n",
    "df_1['Class 2G'] = pd.to_numeric(df_1.iloc[:,17], errors='coerce') \n",
    "df_1['Class 1G'] = pd.to_numeric(df_1.iloc[:,18], errors='coerce') \n",
    "\n",
    "\n",
    "# Print the DataFrame to confirm the new column\n",
    "(df_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2665c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## After tax reduction income\n",
    "\n",
    "# Add the values of two columns (e.g., 'column1' and 'column2') and assign the result to a new column\n",
    "avg_expense = 1530\n",
    "\n",
    "df_2['Class 6G_T'] = pd.to_numeric(df_2.iloc[:,7]) + pd.to_numeric(df_2.iloc[:,8])\n",
    "df_2['Class 5G_T'] = pd.to_numeric(df_2.iloc[:,9]) + pd.to_numeric(df_2.iloc[:,10]) +pd.to_numeric(df_2.iloc[:,11])\n",
    "df_2['Class 4G_T'] = pd.to_numeric(df_2.iloc[:,12]) + pd.to_numeric(df_2.iloc[:,13]) \n",
    "df_2['Class 3G_T'] = pd.to_numeric(df_2.iloc[:,14]) + pd.to_numeric(df_2.iloc[:,15]) +pd.to_numeric(df_2.iloc[:,16])\n",
    "df_2['Class 2G_T'] = pd.to_numeric(df_2.iloc[:,17]) \n",
    "df_2['Class 1G_T'] = pd.to_numeric(df_2.iloc[:,18]) \n",
    "\n",
    "# Print the DataFrame to confirm the new column\n",
    "(df_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1540b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure \"Neighbourhood\" is the index in both DataFrames\n",
    "df_1.set_index('Neighbourhood', inplace=True)  # Modify df_1 in place\n",
    "hos_t.set_index('Neighbourhood', inplace=True)  # Modify hos_t in place\n",
    "# df_1.set_index('Neighbourhood', inplace= False)\n",
    "# hos_t.set_index('Neighbourhood', inplace= False)\n",
    "\n",
    "# Step 2: Select the last 6 columns from df_1\n",
    "last_6_columns_df1 = df_1.iloc[:, -6:]\n",
    "\n",
    "# Step 3: Merge these columns into hos_t based on the \"Neighbourhood\" index\n",
    "hos_t = hos_t.merge(last_6_columns_df1, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Step 4: Reset the index if you want the \"Neighbourhood\" back as a column\n",
    "hos_t.reset_index(inplace=True)\n",
    "\n",
    "# Step 5: Display the updated hos_t DataFrame\n",
    "(hos_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeabe7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have NaN in specific columns \n",
    "final_df = hos_t.dropna(subset=['Class 6G'])\n",
    "\n",
    "# Display the DataFrame after dropping rows\n",
    "(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfb67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Ensure \"Neighbourhood\" is the index in both DataFrames\n",
    "df_2.set_index('Neighbourhood', inplace=True)  # Modify df_1 in place\n",
    "hos_t.set_index('Neighbourhood', inplace=True)  # Modify hos_t in place\n",
    "# df_1.set_index('Neighbourhood', inplace= False)\n",
    "# hos_t.set_index('Neighbourhood', inplace= False)\n",
    "\n",
    "# Step 2: Select the last 6 columns from df_1\n",
    "last_6_columns_df2 = df_2.iloc[:, -6:]\n",
    "\n",
    "# Step 3: Merge these columns into hos_t based on the \"Neighbourhood\" index\n",
    "hos_t = hos_t.merge(last_6_columns_df2, how='left', left_index=True, right_index=True)\n",
    "\n",
    "# Step 4: Reset the index if you want the \"Neighbourhood\" back as a column\n",
    "hos_t.reset_index(inplace=True)\n",
    "\n",
    "# Step 5: Display the updated hos_t DataFrame\n",
    "(hos_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cff156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have NaN in specific columns (e.g., 'Column1', 'Column2', ...)\n",
    "final_df2 = hos_t.dropna(subset=['Class 6G_T'])\n",
    "\n",
    "# Display the DataFrame after dropping rows\n",
    "(final_df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91650836",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df2.to_csv('Toronto_houses_with_crime_income_data_0.5km.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3f692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd27b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
